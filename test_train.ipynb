{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53cb38d5-94d8-4510-9eb5-612ea8d79243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from eprocessing.dataload import ImageDataset\n",
    "from eprocessing.etransforms import Scale, RandCrop, AddAWGN\n",
    "from etrain.trainer import NNTrainer\n",
    "from modelbuild.denoiser import DivergentRestorer\n",
    "from emetrics.metrics import *\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ab6353-df38-480b-b0df-ca64a9df5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PixelFrequencyLayer(nn.Module):\n",
    "    def __init__(self, num_bins=256):\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "        Args:\n",
    "            num_bins (int): Number of bins for the pixel intensity values (default: 256 for 8-bit images).\n",
    "        \"\"\"\n",
    "        super(PixelFrequencyLayer, self).__init__()\n",
    "        self.num_bins = num_bins\n",
    "        self.register_buffer(\"pixel_probabilities\", torch.ones(num_bins) / num_bins)\n",
    "    \n",
    "    def compute_frequencies(self, images):\n",
    "        \"\"\"\n",
    "        Compute pixel intensity frequencies and update probabilities.\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images (batch_size, channels, height, width).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Flatten and compute histogram\n",
    "            flat_pixels = images.flatten()\n",
    "            hist = torch.histc(flat_pixels, bins=self.num_bins, min=0, max=self.num_bins - 1)\n",
    "            \n",
    "            # Normalize histogram to probabilities\n",
    "            total_pixels = flat_pixels.numel()\n",
    "            self.pixel_probabilities = hist / total_pixels\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Transform the input image pixels into probabilities.\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images (batch_size, channels, height, width).\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed images with probabilities.\n",
    "        \"\"\"\n",
    "        # Map pixel values to probabilities\n",
    "        pixel_indices = images.long()  # Ensure pixel values are integers\n",
    "        probabilities = self.pixel_probabilities[pixel_indices]\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "421b7d5e-4d06-47c0-a352-f6d7056c20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelwiseVariance(nn.Module):\n",
    "    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n",
    "        \"\"\"\n",
    "        Custom layer to compute channel-wise variance maps.\n",
    "        \n",
    "        Args:\n",
    "            kernel_size (int): Size of the kernel (assumed square).\n",
    "            stride (int): Stride for the sliding window.\n",
    "            padding (int): Padding to apply to the input.\n",
    "        \"\"\"\n",
    "        super(ChannelwiseVariance, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = nn.ReplicationPad2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute channel-wise variance maps.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Variance map of shape (B, C, H', W').\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Unfold the input to extract patches of shape (B, C, kernel_size*kernel_size, L)\n",
    "        patches = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding\n",
    "        )  # Shape: (B, C * kernel_size^2, L)\n",
    "\n",
    "        # Reshape to (B, C, kernel_size*kernel_size, L)\n",
    "        patches = patches.view(B, C, self.kernel_size**2, -1)\n",
    "\n",
    "        # Compute mean along patch dimension\n",
    "        mean = patches.mean(dim=2, keepdim=True)  # Shape: (B, C, 1, L)\n",
    "\n",
    "        # Compute variance along patch dimension\n",
    "        variance = ((patches - mean) ** 2).mean(dim=2)  # Shape: (B, C, L)\n",
    "\n",
    "        # Reshape back to spatial dimensions\n",
    "        h_out = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        w_out = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        variance_map = variance.view(B, C, h_out, w_out)\n",
    "\n",
    "        return variance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df292218-2bc1-4373-b6e5-d5743be96e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelVarianceLayer(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3):\n",
    "        super(ChannelVarianceLayer, self).__init__()\n",
    "        # Create a 2x2 kernel for each channel, initialize to 1/4 (for mean calculation)\n",
    "        self.kernel = torch.ones((in_channels, 1, 2, 2)) / 4.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the size of the input tensor\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Padding to maintain the same size\n",
    "        padding = (1, 0, 1, 0)\n",
    "        x_padded = F.pad(x, padding, mode='replicate')\n",
    "        \n",
    "        # Compute the squared values for variance calculation\n",
    "        squared_x = x_padded ** 2\n",
    "        \n",
    "        # Apply 2x2 average kernel to compute the mean squared values (i.e., second moment)\n",
    "        mean_squared = F.conv2d(squared_x, self.kernel, stride=1, padding=0, groups=channels)\n",
    "\n",
    "        # Apply 2x2 average kernel to compute the mean (i.e., first moment)\n",
    "        mean = F.conv2d(x_padded, self.kernel, stride=1, padding=0, groups=channels)\n",
    "\n",
    "        # Compute the variance: variance = E[X^2] - (E[X])^2\n",
    "        variance_map = mean_squared - mean ** 2\n",
    "\n",
    "        # Since the variance computation is applied to the padded version, remove the padding\n",
    "        variance_map = variance_map\n",
    "        \n",
    "        return variance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de01748c-f34b-4dfe-a67c-236888ee9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = Path('D:/Projects/datasets/GoPro/GoPro_Large/orig_blur/awgn-0-0/test/y')\n",
    "yp = Path('D:/Projects/datasets/GoPro/GoPro_Large/orig_blur/awgn-0-0/test/y')\n",
    "modelp = Path('D:/Projects/torch-admm-deconv/trained_models/01-01-25/denoiser_gopro_divergent_attention_wconvs_epoch00_vloss0.0380.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be00e34-31ef-46c6-aed6-a55fb0cddaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "im_shape = (256,256)\n",
    "min_std, max_std = 5, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a34000b-5990-4aad-be4c-c08a09d4b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = PSNRMetric(device)\n",
    "ssim = SSIMMetric(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5e8abf-c390-42a5-95ae-02bca7150b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECONV1 = {'kern_size': (),\n",
    "         'max_iters': 100,\n",
    "         'lmbda': 0.02,\n",
    "         'iso': True}\n",
    "DECONV2 = {'kern_size': (),\n",
    "         'max_iters': 100,\n",
    "         'rho': 0.004,\n",
    "         'iso': True}\n",
    "\n",
    "model = DivergentRestorer(3, 2, 3,\n",
    "                          3, 4, 86,\n",
    "                          86, 8,\n",
    "                          output_activation=torch.nn.Sigmoid(), admms=[DECONV1, DECONV2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe0da007-bea2-43f2-8cfa-a27672a96347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = torch.load(modelp, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ddae5dd-3016-42bd-9bc5-abae70e71484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_d['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6fecbe-99d7-41b9-87f3-222705feb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17610b28-a529-4873-9228-52e1aadb4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = ImageDataset(xp, yp, transforms=[Scale(), RandCrop(im_shape), AddAWGN(std_range=(15, 1), both=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77d69e07-495c-4681-bc24-801d80d6e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdt = torch.utils.data.DataLoader(imd, shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084a3e0-a2ad-494c-97d0-740088b02d06",
   "metadata": {},
   "source": [
    "## Load test image and add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fbfae4d0-7720-492c-964c-1000ea8d7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = torchvision.transforms.Resize(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7b10404-3518-431c-b4d1-4a1b0d0841e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_im = torchvision.io.read_image('D:/Projects/torch-admm-deconv/test_imgs/baboon256.png') / 255.0\n",
    "# ref_im = resize(ref_im)\n",
    "ref_im = ref_im[torch.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68758878-ec82-4932-96c0-b617b697a9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2_image = np.transpose(ref_im[0].numpy() * 255, (1, 2, 0))\n",
    "cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite('D:/Projects/torch-admm-deconv/test_imgs/baboon256.png', cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f9c281-de47-4940-968b-e5f07245e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "awgn_gen = AddAWGN(std_range=(15, 16), both=False)\n",
    "noisy_ref = awgn_gen(ref_im, ref_im)\n",
    "noisy_ref = noisy_ref[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff08585-056c-48eb-8b05-3ffc3974d0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2_image = np.transpose(noisy_ref[0].numpy() * 255, (1, 2, 0))\n",
    "cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite('D:/Projects/torch-admm-deconv/test_imgs/ref_noisy.png', cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc16ea08-1672-4ab6-aebb-8801c4865320",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model(noisy_ref.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb5f8ba-344b-471f-9bc3-9070438afa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model_out.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd694df8-6cba-4610-8adf-3a19ac9050a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2_image = np.transpose(model_out[0].to('cpu').detach().numpy() * 255, (1, 2, 0))\n",
    "cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite('D:/Projects/torch-admm-deconv/test_imgs/model_dednoised.png', cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39ed09d4-ceba-49c4-b52f-a8df13eac88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_b = torch.clamp(model_out + 1.72/255, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c900897-7a58-4e1b-a5b6-e73007813b20",
   "metadata": {},
   "source": [
    "### Load ffdnet image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b13c246a-81c8-4dfc-9cb4-9dca4bed04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffndet = torchvision.io.read_image('D:/Projects/torch-admm-deconv/test_imgs/ffdnet.png') / 255.0\n",
    "ffndet = ffndet[torch.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92af8f6d-48c5-4296-9d56-f5e803919872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.2484, device='cuda:0', grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psnr(model_out, ref_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb07932-4e07-4ed0-b801-7301a123e272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8669, device='cuda:0', grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim(model_out, ref_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8e15090-07ea-4997-8695-316e17fe1936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26.5737, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psnr(ffndet, ref_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a4c56b4-aea1-4967-a0c8-bdf58b01acdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7555, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssim(ffndet, ref_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5bb364-90f5-425f-ab77-68cfa94b7855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
