{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb38d5-94d8-4510-9eb5-612ea8d79243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from eprocessing.dataload import ImageDataset\n",
    "from eprocessing.etransforms import Scale, RandCrop, AddAWGN\n",
    "from etrain.trainer import NNTrainer\n",
    "from modelbuild.denoiser import DivergentRestorer\n",
    "from emetrics.metrics import *\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab6353-df38-480b-b0df-ca64a9df5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PixelFrequencyLayer(nn.Module):\n",
    "    def __init__(self, num_bins=256):\n",
    "        \"\"\"\n",
    "        Initialize the layer.\n",
    "        Args:\n",
    "            num_bins (int): Number of bins for the pixel intensity values (default: 256 for 8-bit images).\n",
    "        \"\"\"\n",
    "        super(PixelFrequencyLayer, self).__init__()\n",
    "        self.num_bins = num_bins\n",
    "        self.register_buffer(\"pixel_probabilities\", torch.ones(num_bins) / num_bins)\n",
    "    \n",
    "    def compute_frequencies(self, images):\n",
    "        \"\"\"\n",
    "        Compute pixel intensity frequencies and update probabilities.\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images (batch_size, channels, height, width).\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Flatten and compute histogram\n",
    "            flat_pixels = images.flatten()\n",
    "            hist = torch.histc(flat_pixels, bins=self.num_bins, min=0, max=self.num_bins - 1)\n",
    "            \n",
    "            # Normalize histogram to probabilities\n",
    "            total_pixels = flat_pixels.numel()\n",
    "            self.pixel_probabilities = hist / total_pixels\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Transform the input image pixels into probabilities.\n",
    "        Args:\n",
    "            images (torch.Tensor): Input images (batch_size, channels, height, width).\n",
    "        Returns:\n",
    "            torch.Tensor: Transformed images with probabilities.\n",
    "        \"\"\"\n",
    "        # Map pixel values to probabilities\n",
    "        pixel_indices = images.long()  # Ensure pixel values are integers\n",
    "        probabilities = self.pixel_probabilities[pixel_indices]\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b7d5e-4d06-47c0-a352-f6d7056c20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelwiseVariance(nn.Module):\n",
    "    def __init__(self, kernel_size: int, stride: int = 1, padding: int = 0):\n",
    "        \"\"\"\n",
    "        Custom layer to compute channel-wise variance maps.\n",
    "        \n",
    "        Args:\n",
    "            kernel_size (int): Size of the kernel (assumed square).\n",
    "            stride (int): Stride for the sliding window.\n",
    "            padding (int): Padding to apply to the input.\n",
    "        \"\"\"\n",
    "        super(ChannelwiseVariance, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = nn.ReplicationPad2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute channel-wise variance maps.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Variance map of shape (B, C, H', W').\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # Unfold the input to extract patches of shape (B, C, kernel_size*kernel_size, L)\n",
    "        patches = F.unfold(\n",
    "            x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding\n",
    "        )  # Shape: (B, C * kernel_size^2, L)\n",
    "\n",
    "        # Reshape to (B, C, kernel_size*kernel_size, L)\n",
    "        patches = patches.view(B, C, self.kernel_size**2, -1)\n",
    "\n",
    "        # Compute mean along patch dimension\n",
    "        mean = patches.mean(dim=2, keepdim=True)  # Shape: (B, C, 1, L)\n",
    "\n",
    "        # Compute variance along patch dimension\n",
    "        variance = ((patches - mean) ** 2).mean(dim=2)  # Shape: (B, C, L)\n",
    "\n",
    "        # Reshape back to spatial dimensions\n",
    "        h_out = (H + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        w_out = (W + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        variance_map = variance.view(B, C, h_out, w_out)\n",
    "\n",
    "        return variance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df292218-2bc1-4373-b6e5-d5743be96e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelVarianceLayer(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3):\n",
    "        super(ChannelVarianceLayer, self).__init__()\n",
    "        # Create a 2x2 kernel for each channel, initialize to 1/4 (for mean calculation)\n",
    "        self.kernel = torch.ones((in_channels, 1, 2, 2)) / 4.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the size of the input tensor\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Padding to maintain the same size\n",
    "        padding = (1, 0, 1, 0)\n",
    "        x_padded = F.pad(x, padding, mode='replicate')\n",
    "        \n",
    "        # Compute the squared values for variance calculation\n",
    "        squared_x = x_padded ** 2\n",
    "        \n",
    "        # Apply 2x2 average kernel to compute the mean squared values (i.e., second moment)\n",
    "        mean_squared = F.conv2d(squared_x, self.kernel, stride=1, padding=0, groups=channels)\n",
    "\n",
    "        # Apply 2x2 average kernel to compute the mean (i.e., first moment)\n",
    "        mean = F.conv2d(x_padded, self.kernel, stride=1, padding=0, groups=channels)\n",
    "\n",
    "        # Compute the variance: variance = E[X^2] - (E[X])^2\n",
    "        variance_map = mean_squared - mean ** 2\n",
    "\n",
    "        # Since the variance computation is applied to the padded version, remove the padding\n",
    "        variance_map = variance_map\n",
    "        \n",
    "        return variance_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01748c-f34b-4dfe-a67c-236888ee9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = Path('D:/Projects/datasets/GoPro/GoPro_Large/orig_blur/awgn-0-0/test/y')\n",
    "yp = Path('D:/Projects/datasets/GoPro/GoPro_Large/orig_blur/awgn-0-0/test/y')\n",
    "modelp = Path('D:/Projects/torch-admm-deconv/trained_models/denoiser_gopro_divergent_attention_epoch72_vloss0.0397.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be00e34-31ef-46c6-aed6-a55fb0cddaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "im_shape = (256,256)\n",
    "min_std, max_std = 5, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34000b-5990-4aad-be4c-c08a09d4b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr = PSNRMetric(device)\n",
    "ssim = SSIMMetric(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e8abf-c390-42a5-95ae-02bca7150b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECONV1 = {'kern_size': (),\n",
    "         'max_iters': 100,\n",
    "         'lmbda': 0.02,\n",
    "         'iso': True}\n",
    "DECONV2 = {'kern_size': (),\n",
    "         'max_iters': 100,\n",
    "         'rho': 0.004,\n",
    "         'iso': True}\n",
    "\n",
    "model = DivergentRestorer(3, 2, 3,\n",
    "                          3, 4, 128,\n",
    "                          128, 8,\n",
    "                          output_activation=torch.nn.Sigmoid(), admms=[DECONV1, DECONV2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0da007-bea2-43f2-8cfa-a27672a96347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = torch.load(modelp, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddae5dd-3016-42bd-9bc5-abae70e71484",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(model_d['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fecbe-99d7-41b9-87f3-222705feb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17610b28-a529-4873-9228-52e1aadb4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd = ImageDataset(xp, yp, transforms=[Scale(), RandCrop(im_shape), AddAWGN(std_range=(25, 30), both=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d69e07-495c-4681-bc24-801d80d6e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdt = torch.utils.data.DataLoader(imd, shuffle=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52525fca-d081-4aa6-9620-fd6a282ac9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imx, imy = imd[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573dd1e-abd0-4b38-bd86-d2b5350965bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imxx = imx[:, :, :][torch.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095210b-f528-4acf-b039-159d713ee343",
   "metadata": {},
   "outputs": [],
   "source": [
    "imxx = imxx.expand(1,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a78e5f-66fb-4b90-8f49-0fd4cce79856",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(ref.to(device))\n",
    "out = out[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcee3b6-9759-4ed7-9047-72e935dbb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "varmap = ChannelVarianceLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563c565-e98d-4e02-9cbc-a8dfb88c7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "varm = varmap(imxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea2bf22-c6ad-42fd-b046-ff8d1e7d3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "varm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c8d9b-68e4-4671-91a0-6d78964a318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_image = np.transpose(ref[0].numpy() * 255, (1, 2, 0))\n",
    "cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite('ffdin.png', cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1255a29-e576-4d3d-901d-790e8cd60d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ref.permute((1,2,0)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067a60c-4892-4991-8c3a-6a654575e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out.permute((1,2,0)).detach().numpy())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc79309-bfda-4083-9218-be11fce939df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imy.permute((1,2,0)).detach().numpy())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5733164e-09c7-4261-9c40-3604b95e9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ffdout.permute((1,2,0)).detach().numpy())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af8f6d-48c5-4296-9d56-f5e803919872",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(ffdnet, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445d3d5-7f86-4ea7-b9d7-8ead5b53d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(out[torch.newaxis,...], ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb07932-4e07-4ed0-b801-7301a123e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(out[torch.newaxis,...], ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e15090-07ea-4997-8695-316e17fe1936",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(ffdout[torch.newaxis,...] / 255, imy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c56b4-aea1-4967-a0c8-bdf58b01acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(ffdout[torch.newaxis,...] / 255, imy[torch.newaxis,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5bb364-90f5-425f-ab77-68cfa94b7855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100bf7c-b435-47d9-a647-97933b25d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffdnet = torchvision.io.read_image('D:/Projects/torch-admm-deconv/ffdnetout.png') / 255.0\n",
    "ref = torchvision.io.read_image('D:/Projects/torch-admm-deconv/ref.png') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e9c73-3cb4-4a31-b04d-535d21a25ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffdnet = ffdnet[torch.newaxis,...]\n",
    "ref = ref[torch.newaxis,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ebb5d-4e77-42ae-b3da-afdfb4068cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffdnet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9fffb-01f3-4f54-83b4-e2cb9725a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(out[torch.newaxis,...], imy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a01bb-394e-4d7a-870b-4f021907ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(out[torch.newaxis,...], imy[torch.newaxis,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44eef8-bc1d-4424-bf0f-186aee578c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(ffdout[torch.newaxis,...] / 255, imy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba85948-62d1-4d6d-9d2f-f2b84c35ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim(ffdout[torch.newaxis,...] / 255, imy[torch.newaxis,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6a0b2-1be3-4fc4-b526-d1d56ecbd5be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
